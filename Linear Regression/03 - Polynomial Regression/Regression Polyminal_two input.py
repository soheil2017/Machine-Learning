#https://realpython.com/linear-regression-in-python/#beyond-linear-regression
#Polynomial Regression With scikit-learn (two column input)
#ğ‘“(ğ‘¥â‚, ğ‘¥â‚‚) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + ğ‘â‚‚ğ‘¥â‚‚ + ğ‘â‚ƒğ‘¥â‚Â² + ğ‘â‚„ğ‘¥â‚ğ‘¥â‚‚ + ğ‘â‚…ğ‘¥â‚‚Â². polynominal for two inputs

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures


x = np.array([[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]])
y = np.array([4, 5, 20, 14, 32, 22, 38, 43])
x, y = np.array(x), np.array(y)

# Step 2b: Transform input data
x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)

# Step 3: Create a model and fit it
model = LinearRegression().fit(x_, y)

# Step 4: Get results
r_sq = model.score(x_, y)
intercept, coefficients = model.intercept_, model.coef_

# Step 5: Predict
y_pred = model.predict(x_)

print(('coefficient of determination:', r_sq))
print("="*40)

print('intercept:', intercept)
print("="*40)

print('coefficients:', coefficients, sep='\n')
print("="*40)

print('predicted response:', y_pred, sep='\n')
print("="*40)

#------------------------------------------------------------------------
